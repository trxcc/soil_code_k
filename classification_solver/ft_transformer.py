import math
import os
import pickle
from typing import Dict

import delu
import numpy as np
import torch
import torch.nn.functional as F
from rtdl_revisiting_models import FTTransformer
from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import QuantileTransformer
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm

from .base import ClassificationModel


class FTTransformerModel(ClassificationModel):
    def __init__(
        self,
        name: str = "FTTransformer",
        task_name: str = "",
        seed: int = 2024,
        model_dir=None,
        results_dir=None,
        n_dim=None,
        n_classes=None,
    ):
        super().__init__(name)
        self.X_val = None
        self.y_val = None
        self.model = None
        self.model_dir = model_dir
        self.results_dir = results_dir
        self.batch_size = 256
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.n_cont_features = n_dim
        self.n_classes = n_classes
        
        # åˆå§‹åŒ–æ¨¡åž‹
        self.model = FTTransformer(
            n_cont_features=self.n_cont_features,
            cat_cardinalities=[],
            d_out=self.n_classes,  # è¾“å‡ºç»´åº¦ä¸ºç±»åˆ«æ•°
            **FTTransformer.get_default_kwargs(),
        ).to(self.device)

    def fit(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        load_model: bool = False,
        model_path_if_load=None,
        optimize_hyperparams: bool = False,
        optimize_method=None,
        save_model: bool = False,
    ) -> None:
        # åˆ†å‰²éªŒè¯é›†
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.1, random_state=2023
        )
        
        # æ•°æ®é¢„å¤„ç†
        self.data = self._data_preprocess(X_train, y_train, X_val, y_val)

        if load_model:
            assert model_path_if_load is not None
            self.load(model_path_if_load)
            self.is_trained = True
            return

        # ä¼˜åŒ–å™¨è®¾ç½®
        self.optimizer = torch.optim.AdamW(
            self.model.make_parameter_groups(), lr=1e-4, weight_decay=1e-5
        )

        # å®šä¹‰æ¨¡åž‹åº”ç”¨å‡½æ•°å’ŒæŸå¤±å‡½æ•°
        self.apply_fn = lambda batch, model: model(batch["x_cont"], None)
        self.loss_fn = F.cross_entropy

        # è®­ç»ƒå‚æ•°è®¾ç½®
        n_epochs = 500
        batch_size = 128
        epoch_size = math.ceil(len(X_train) / batch_size)

        # è®­ç»ƒè¿½è¸ª
        timer = delu.tools.Timer()
        best = {
            "val": -math.inf,
            "epoch": -1,
            "model_state_dict": None,
        }

        print(f"Device: {self.device.type.upper()}")
        print("-" * 88 + "\n")
        timer.run()

        # è®­ç»ƒå¾ªçŽ¯
        for epoch in range(n_epochs):
            losses = []
            for batch in tqdm(
                delu.iter_batches(self.data["train"], batch_size, shuffle=True),
                desc=f"Epoch {epoch}",
                total=epoch_size,
            ):
                self.model.train()
                self.optimizer.zero_grad()
                loss = self.loss_fn(self.apply_fn(batch, self.model), batch["y"].long())
                losses.append(loss.item())
                loss.backward()
                self.optimizer.step()

            # è¯„ä¼°å½“å‰epoch
            val_score = self.evaluate_model("val")
            
            if val_score > best["val"]:
                print(f"ðŸŒ¸ æ–°çš„æœ€ä½³æ¨¡åž‹! ç»¼åˆå¾—åˆ†: {val_score:.4f} ðŸŒ¸")
                best = {
                    "val": val_score,
                    "epoch": epoch,
                    "model_state_dict": self.model.state_dict(),
                }
                if save_model:
                    self.save()
            print(f"Epoch {epoch}: éªŒè¯é›†ç»¼åˆå¾—åˆ† = {val_score:.4f}")

        # åŠ è½½æœ€ä½³æ¨¡åž‹
        self.model.load_state_dict(best["model_state_dict"])
        self.is_trained = True

    def predict(self, X_test: np.ndarray) -> np.ndarray:
        # é¢„å¤„ç†æ•°æ®
        X_test = self.preprocessing.transform(X_test)
        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
        
        # åˆ›å»º DataLoader
        test_dataset = TensorDataset(X_test_tensor)
        test_loader = DataLoader(
            test_dataset, batch_size=self.batch_size, shuffle=False
        )
        
        predictions = []
        self.model.eval()
        with torch.no_grad():
            for batch in test_loader:
                batch_X = batch[0].to(self.device)
                logits = self.model(batch_X, None)
                pred = torch.argmax(logits, dim=1).cpu().numpy()
                predictions.append(pred)
                
        return np.concatenate(predictions, axis=0)

    def predict_proba(self, X_test: np.ndarray) -> np.ndarray:
        # é¢„å¤„ç†æ•°æ®
        X_test = self.preprocessing.transform(X_test)
        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
        
        # åˆ›å»º DataLoader
        test_dataset = TensorDataset(X_test_tensor)
        test_loader = DataLoader(
            test_dataset, batch_size=self.batch_size, shuffle=False
        )
        
        probabilities = []
        self.model.eval()
        with torch.no_grad():
            for batch in test_loader:
                batch_X = batch[0].to(self.device)
                logits = self.model(batch_X, None)
                probs = F.softmax(logits, dim=1).cpu().numpy()
                probabilities.append(probs)
                
        return np.concatenate(probabilities, axis=0)

    @torch.no_grad()
    def evaluate_model(self, part: str) -> float:
        self.model.eval()
        eval_batch_size = 8096
        
        all_predictions = []
        all_probabilities = []
        all_true_labels = []
        
        for batch in delu.iter_batches(self.data[part], eval_batch_size):
            logits = self.apply_fn(batch, self.model)
            probabilities = F.softmax(logits, dim=1)
            predictions = torch.argmax(logits, dim=1)
            
            all_predictions.extend(predictions.cpu().numpy())
            all_probabilities.extend(probabilities.cpu().numpy())
            all_true_labels.extend(batch["y"].cpu().numpy())
        
        all_predictions = np.array(all_predictions)
        all_probabilities = np.array(all_probabilities)
        all_true_labels = np.array(all_true_labels)
        
        # è®¡ç®—å„ä¸ªæŒ‡æ ‡
        accuracy = accuracy_score(all_true_labels, all_predictions)
        
        # å¯¹äºŽäºŒåˆ†ç±»é—®é¢˜ï¼Œåªä½¿ç”¨æ­£ç±»çš„æ¦‚çŽ‡
        if self.n_classes == 2:
            roc_auc = roc_auc_score(all_true_labels, all_probabilities[:, 1])
            average_precision = average_precision_score(all_true_labels, all_probabilities[:, 1])
        else:
            roc_auc = roc_auc_score(all_true_labels, all_probabilities, multi_class='ovr')
            average_precision = average_precision_score(all_true_labels, all_probabilities, average='macro')
        
        # ä½¿ç”¨ä¸Ž LightGBM ç›¸åŒçš„åŠ æƒæ–¹å¼
        final_score = (
            accuracy * 0.4 +
            roc_auc * 0.3 +
            average_precision * 0.3
        )
        
        return final_score

    def _data_preprocess(self, X_train, y_train, X_val, y_val):
        data_numpy = {
            "train": {"x_cont": X_train, "y": y_train},
            "val": {"x_cont": X_val, "y": y_val},
        }

        X_cont_train_numpy = data_numpy["train"]["x_cont"]
        self.preprocessing = QuantileTransformer(
            n_quantiles=max(min(len(X_cont_train_numpy) // 30, 1000), 10),
            output_distribution="normal",
            subsample=10**9,
        ).fit(X_cont_train_numpy)

        if self.results_dir is not None:
            with open(os.path.join(self.results_dir, "preprocessing.pkl"), "wb+") as f:
                pickle.dump(obj=self.preprocessing, file=f)

        for part in data_numpy:
            data_numpy[part]["x_cont"] = self.preprocessing.transform(
                data_numpy[part]["x_cont"]
            )

        data = {
            part: {
                k: torch.as_tensor(v, device=self.device, dtype=torch.float32)
                for k, v in data_numpy[part].items()
            }
            for part in data_numpy
        }

        return data

    def save(self, save_path=None) -> None:
        assert save_path is not None or self.model_dir is not None
        if save_path is None:
            save_path = self.model_dir
        self.model = self.model.to("cpu")
        checkpoint = self.model.state_dict()
        torch.save(checkpoint, os.path.join(save_path, "FTTransformer_classifier.pt"))
        self.model = self.model.to(self.device)

    def load(self, model_dir):
        model_path = os.path.join(model_dir, "FTTransformer_classifier.pt")
        assert os.path.exists(model_path)
        self.model.load_state_dict(torch.load(model_path))
        self.model.to(self.device) 